# search.py
"""
ç½‘ç»œæœç´¢æ¨¡å—
"""
import time
from playwright.sync_api import sync_playwright
import trafilatura
# from bs4 import BeautifulSoup # å¦‚æœtrafilaturaè¶³å¤Ÿï¼Œå¯ä»¥ä¸éœ€è¦bs4
from colorama import Fore, Style

from config import HEADLESS, HIDE_WINDOW, MAX_SEARCH_RESULTS, MAX_PAGES_TO_SCAN

class SearchEngine:
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.context = None

    def start(self):
        self.playwright = sync_playwright().start()
        args = [
                   "--disable-blink-features=AutomationControlled", 
                   "--start-maximized",
                ]  
        if HIDE_WINDOW:
            args.extend([
                    "--headless=new", # Playwrightæ¨èçš„æ–°headlessæ¨¡å¼
                   "--no-sandbox",
                   "--window-position=-2000,-2000"]) # å°†çª—å£å®šä½åœ¨å±å¹•å¤–
        else:
            args.extend([
                   "--window-position=0, 0"])
        self.browser = self.playwright.chromium.launch(
            headless=HEADLESS, 
            args=args 
        )
        self.context = self.browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/125.0.0.0 Safari/537.36",
            locale="zh-CN"
        )
        print(f"{Fore.GREEN}>> ğŸŒ æµè§ˆå™¨å¯åŠ¨æˆåŠŸ.{Style.RESET_ALL}")

    def stop(self):
        if self.context: self.context.close()
        if self.browser: self.browser.close()
        if self.playwright: self.playwright.stop()
        print(f"{Fore.GREEN}>> ğŸŒ æµè§ˆå™¨å·²å…³é—­.{Style.RESET_ALL}")

    def search(self, query):
        """
        æ ¹æ®æˆªå›¾åé¦ˆä¼˜åŒ–çš„ç¿»é¡µæœç´¢ï¼š
        1. æ¨¡æ‹Ÿæ»šåŠ¨åˆ°åº•éƒ¨è§¦å‘"æ›´å¤šç»“æœ"æŒ‰é’®ã€‚
        2. ç²¾ç¡®ç‚¹å‡»"æ›´å¤šç»“æœ"æŒ‰é’®ã€‚
        3. å¢åŠ ç‚¹å‡»åçš„ç¼“å†²ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½ã€‚
        """
        page = self.context.new_page()
        docs = []
        unique_links = []
        seen_urls = set()

        max_scan_pages = MAX_PAGES_TO_SCAN # å…è®¸ç¿»é¡µçš„ä¸Šé™ï¼Œä»é…ç½®ä¸­è¯»å–

        try:
            print(f"{Fore.YELLOW}>> ğŸŒ æ­£åœ¨è”ç½‘æ£€ç´¢å¹¶æ¨¡æ‹Ÿç¿»é¡µ: {query}{Style.RESET_ALL}")
            page.goto(f"https://duckduckgo.com/?q={query}&ia=web", timeout=20000)
            
            pages_clicked = 0
            while len(unique_links) < MAX_SEARCH_RESULTS and pages_clicked < max_scan_pages:
                # 1. ç­‰å¾…åˆå§‹ç»“æœåŠ è½½
                try:
                    page.wait_for_selector('[data-testid="result"]', timeout=8000)
                except:
                    print(f"{Fore.LIGHTBLACK_EX}   [-] æœªæ‰¾åˆ°æœç´¢ç»“æœæˆ–åŠ è½½è¶…æ—¶ï¼Œåœæ­¢ç¿»é¡µã€‚{Style.RESET_ALL}")
                    break

                # 2. æå–å½“å‰å·²åŠ è½½çš„æ‰€æœ‰é“¾æ¥
                # ä¼˜å…ˆä½¿ç”¨data-testidå±æ€§ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°h2 a
                locs = page.locator('a[data-testid="result-title-a"]').all()
                if not locs:
                    locs = page.locator('article h2 a').all()

                for l in locs:
                    try:
                        url = l.get_attribute('href')
                        title = l.inner_text().strip()
                        if url and "http" in url and url not in seen_urls:
                            # è¿‡æ»¤æ‰DuckDuckGoè‡ªèº«çš„é“¾æ¥
                            if "duckduckgo.com" not in url and "start.duckduckgo.com" not in url:
                                unique_links.append({'title': title, 'url': url})
                                seen_urls.add(url)
                    except Exception as e:
                        # å¿½ç•¥æ— æ³•è·å–å±æ€§çš„å…ƒç´ 
                        # print(f"{Fore.RED}   [!] æå–é“¾æ¥æ—¶å‡ºé”™: {e}{Style.RESET_ALL}")
                        pass
                
                # å¦‚æœå½“å‰æ•°é‡å·²ç»è¾¾æ ‡ï¼Œæå‰é€€å‡º
                if len(unique_links) >= MAX_SEARCH_RESULTS:
                    break

                # 3. æ¨¡æ‹Ÿä½ åœ¨æˆªå›¾ä¸­çš„æ“ä½œï¼šæ»‘åˆ°åº•éƒ¨ç‚¹å‡»
                print(f"{Fore.CYAN}   [ç¿»é¡µ] å·²è·å– {len(unique_links)} æ¡ï¼Œæ­£åœ¨å‘ä¸‹æ»‘åŠ¨å¯»æ‰¾æŒ‰é’®...{Style.RESET_ALL}")
                
                # æ»šåŠ¨åˆ°åº•éƒ¨
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                time.sleep(1.5) # ç­‰å¾…æŒ‰é’®æ¸²æŸ“

                # å°è¯•ç‚¹å‡»"æ›´å¤šç»“æœ"æŒ‰é’® (æ ¹æ®æˆªå›¾æ–‡æœ¬åŒ¹é…)
                btn_selectors = [
                    "button#more-results",
                    "text='æ›´å¤šç»“æœ'",
                    "text='More Results'",
                    "a.result--more__btn"
                ]
                
                clicked = False
                for selector in btn_selectors:
                    btn = page.locator(selector).first
                    if btn.count() > 0 and btn.is_visible() and btn.is_enabled():
                        try:
                            btn.scroll_into_view_if_needed() # ç¡®ä¿å¯è§
                            btn.click()
                            pages_clicked += 1
                            print(f"{Fore.CYAN}   [âˆš] æˆåŠŸç‚¹å‡»â€œæ›´å¤šç»“æœâ€ (ç¬¬ {pages_clicked} æ¬¡){Style.RESET_ALL}")
                            clicked = True
                            # å…³é”®ï¼šç‚¹å‡»åå¿…é¡»ç­‰ä¸€ä¼šå„¿ï¼Œè®©æ–°ç»“æœåˆ·å‡ºæ¥
                            time.sleep(2.5) 
                            break
                        except Exception as click_err:
                            # print(f"{Fore.RED}   [!] ç‚¹å‡» {selector} å¤±è´¥: {click_err}{Style.RESET_ALL}")
                            continue
                
                # å¦‚æœæ²¡æ‰¾åˆ°æŒ‰é’®ï¼Œæˆ–è€…ç‚¹å‡»åé“¾æ¥æ•°æ²¡å¢åŠ ï¼Œè¯´æ˜å·²ç»åˆ°åº•äº†
                if not clicked:
                    print(f"{Fore.LIGHTBLACK_EX}   [-] æœªå‘ç°æ›´å¤šç»“æœæŒ‰é’®æˆ–æ— æ³•ç‚¹å‡»ï¼Œæœç´¢ç»“æŸã€‚{Style.RESET_ALL}")
                    break
                
                # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœç‚¹å‡»äº†ä½†ç»“æœæ²¡å˜ï¼Œå¯èƒ½æ˜¯è¢«æ‹¦æˆªæˆ–çœŸçš„æ²¡äº†
                # ç®€å•æ¯”è¾ƒæœ¬æ¬¡å¾ªç¯å¼€å§‹å’Œç»“æŸæ—¶ unique_links çš„æ•°é‡
                new_links_after_click = len(unique_links)
                if len(unique_links) <= new_links_after_click: # ä¸¥æ ¼æ¥è¯´ï¼Œåº”è¯¥æ¯”å¯¹æ–°æŠ“å–çš„ locs æ•°é‡ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œè¿™æ ·ä¹Ÿå¯
                     # å†æ¬¡å°è¯•æ»šåŠ¨ä¸€æ¬¡çœ‹æ˜¯å¦è§¦å‘ (åº”å¯¹æ‡’åŠ è½½)
                    page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                    time.sleep(1.0)
                    # é‡æ–°æŠ“å–ä¸€æ¬¡é“¾æ¥ï¼Œçœ‹æ˜¯å¦æœ‰æ–°å¢
                    locs_after_scroll = page.locator('a[data-testid="result-title-a"]').all()
                    if not locs_after_scroll:
                        locs_after_scroll = page.locator('article h2 a').all()
                    
                    temp_new_count = 0
                    for l in locs_after_scroll:
                        try:
                            url = l.get_attribute('href')
                            title = l.inner_text().strip()
                            if url and "http" in url and url not in seen_urls:
                                if "duckduckgo.com" not in url and "start.duckduckgo.com" not in url:
                                    unique_links.append({'title': title, 'url': url})
                                    seen_urls.add(url)
                                    temp_new_count += 1
                        except:
                            pass
                    
                    if temp_new_count == 0:
                        print(f"{Fore.LIGHTBLACK_EX}   [-] ç‚¹å‡»åæˆ–äºŒæ¬¡æ»šåŠ¨åæœªå‘ç°æ–°å†…å®¹åŠ è½½ï¼Œå¯èƒ½å·²è¾¾ç»“æœä¸Šé™ã€‚{Style.RESET_ALL}")
                        break


            # 4. æ·±åº¦é˜…è¯»ç¯èŠ‚ (æˆªå–æŒ‡å®šæ•°é‡)
            final_links = unique_links[:MAX_SEARCH_RESULTS]
            print(f"{Fore.YELLOW}>> ğŸ” æœ€ç»ˆå†³å®šæ·±åº¦é˜…è¯» {len(final_links)} ç¯‡ç½‘é¡µåŸæ–‡...{Style.RESET_ALL}")

            for i, link in enumerate(final_links, 1):
                try:
                    print(f"[{i}/{len(final_links)}] æ­£åœ¨è¯»å–: {link['title'][:30].strip()}...", end="", flush=True)
                    page.goto(link['url'], timeout=15000, wait_until="domcontentloaded")
                    
                    # æ¨¡æ‹Ÿé˜…è¯»æ»šåŠ¨
                    page.mouse.wheel(0, 2000)
                    time.sleep(1)
                    
                    # ä½¿ç”¨trafilaturaæå–ä¸»è¦å†…å®¹
                    text = trafilatura.extract(page.content()) or page.inner_text("body")
                    
                    # ç®€å•æ¸…æ´—å¹¶æˆªæ–­
                    clean = "\n".join([t.strip() for t in text.split('\n') if len(t.strip()) > 10])
                    clean = clean[:2500] # æˆªæ–­ï¼Œé¿å…è¿‡é•¿
                    
                    if len(clean) > 50:
                        docs.append(f"æ¥æº:ã€Š{link['title']}ã€‹\nURL: {link['url']}\nå†…å®¹:{clean}\n")
                        print(f"{Fore.GREEN} âˆš{Style.RESET_ALL}")
                        print(f"{Fore.LIGHTBLACK_EX}   ğŸ“ æ‘˜è¦: {clean[:80].replace(chr(10), ' ')}...{Style.RESET_ALL}")
                    else:
                        print(f"{Fore.RED} x (å†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º){Style.RESET_ALL}")
                except Exception as e:
                    print(f"{Fore.RED} x (åŠ è½½æˆ–æå–å¤±è´¥: {e}){Style.RESET_ALL}")
                    continue

        except Exception as e:
            print(f"{Fore.RED}æœç´¢å¼‚å¸¸: {e}{Style.RESET_ALL}")
        finally: 
            page.close()
        return docs
